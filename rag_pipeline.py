# -*- coding: utf-8 -*-
"""rag_pipeline.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eSv06B960kEeK0jwPYfQAlOrQXWZ6P1T
"""

# RAG Pipeline for Medical Document QA

import os
import torch
import requests
import dotenv
import chromadb

from dotenv import load_dotenv
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from chromadb.config import Settings
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain.prompts import ChatPromptTemplate
from transformers import pipeline
from langchain.llms import HuggingFacePipeline
from langchain_core.output_parsers import StrOutputParser

# Load environment variables
load_dotenv()
hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Download and load PDF
pdf_url = "https://www.birmingham.ac.uk/Documents/college-mds/trials/bctu/E-MOTIVE/E-MOTIVE%20protocol%20v2.0%20clean.pdf"
pdf_path = "emotive_protocol.pdf"
if not os.path.exists(pdf_path):
    response = requests.get(pdf_url)
    with open(pdf_path, "wb") as f:
        f.write(response.content)

loader = PyPDFLoader(pdf_path)
documents = loader.load()

# Split into chunks
text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)
chunks = text_splitter.split_documents(documents)
chunk_texts = [chunk.page_content for chunk in chunks]

# Embedding setup
def embed_documents(docs: list[str]) -> list[list[float]]:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": device},
    )
    return model.embed_documents(docs)

# Vector DB setup and insert
def insert_documents(collection, docs: list[str]):
    next_id = collection.count()
    for i in range(0, len(docs), 10):
        batch = docs[i:i+10]
        embeddings = embed_documents(batch)
        ids = [f"doc_{next_id + j}" for j in range(len(batch))]
        collection.add(embeddings=embeddings, ids=ids, documents=batch)
        next_id += len(batch)

chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))
collection = chroma_client.get_or_create_collection(name="emotive_documents")
insert_documents(collection, chunk_texts)

# Retrieval logic
def search_documents(collection, query: str, top_k: int = 5):
    query_embedding = embed_documents([query])[0]
    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)
    return results["documents"][0]

def retrieve_context(query: str) -> str:
    docs = search_documents(collection, query)
    return "\n".join(docs)

retriever = RunnableLambda(retrieve_context)

# Prompt template
template = """
You are a medical assistant trained in interpreting clinical protocols.
Use the retrieved context from the E-MOTIVE protocol to answer the following question accurately.
If the protocol does not provide enough information, respond by saying "The E-MOTIVE protocol does not specify this."
Focus on evidence-based practices for managing postpartum hemorrhage (PPH), and limit your response to three concise sentences.

Question: {question}
Context: {context}
Answer:
"""
prompt = ChatPromptTemplate.from_template(template)

# Load FLAN-T5 model
pipe = pipeline("text2text-generation", model="google/flan-t5-base")
llm = HuggingFacePipeline(pipeline=pipe)

# Build RAG chain
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Example Query
query = "What is the first step recommended in E-MOTIVE for managing postpartum hemorrhage?"
result = rag_chain.invoke(query)
print(result)